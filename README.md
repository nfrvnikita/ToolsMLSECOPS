# Инструменты для безопасности моделей машинного обучения

## ART от IBM
ART (Adversarial Robustness Toolbox) - мощный инструмент для создания различных типов атак на модели машинного обучения. Поддерживает:
- **Evasion атаки**: злоумышленник пытается создать у сети оптическую (слуховую, смысловую) иллюзию.
- **Poisoning атаки**: манипулируют данными обучения, чтобы повредить модель.
- **Extraction атаки**: извлекают информацию о модели.
- **Adversarial атаки**: манипулируют данными, которые поступают в продовую модель.
- **Inference атаки**: атакуют конфиденциальность входных данных.
- ***Также можно протестировать и использовать встроенные методы защиты.**

## TextAttack
TextAttack - специализированный инструмент для создания adversarial данных в NLP задачах. Позволяет:
- Генерировать adversarial примеры для атаки на NLP модели.
- Использовать эти примеры для тестирования и улучшения устойчивости моделей.

## Foolbox
Foolbox - библиотека для создания adversarial данных для атак на модели машинного обучения. Основные возможности:
- Создание и применение adversarial примеров для тестирования моделей.
- Оценка устойчивости моделей к различным видам атак.

## modelscan
modelscan - лаконичный инструмент для проверки моделей на наличие уязвимостей в области:
- Сериализации моделей.
- Инъекций кода и других уязвимостей безопасности.

## safetensors
safetensors - инструмент для безопасного сохранения весов моделей. Основные преимущества:
- Предотвращает случайные или преднамеренные модификации весов.
- Обеспечивает безопасность и целостность сохраненных данных.

## ml_privacy_meter
ml_privacy_meter - инструмент для оценки конфиденциальности моделей машинного обучения. **Пока не тестировал, слишком сложный инструмент для меня, нужно больше времени**

## mindgard cli
mindgard cli - инструмент с интеграцией в mlops пайплайн:
- Есть возможность установить максимальное кол-во запросов, которые мы хотим направлять в модель одновременно.
- Можно установить трешхолды на риски `--risk-threshold 50`.
