# Инструменты для безопасности моделей машинного обучения

## ART от IBM
ART (Adversarial Robustness Toolbox) - мощный инструмент для создания различных типов атак на модели машинного обучения. Поддерживает:
- **Evasion атаки**: вводят противника, заставляющего модель делать ошибки.
- **Poisoning атаки**: манипулируют данными обучения, чтобы повредить модель.
- **Extraction атаки**: извлекают информацию о модели.
- **Inference атаки**: атакуют конфиденциальность входных данных.
Также можно протестировать и использовать встроенные методы защиты.

## TextAttack
TextAttack - специализированный инструмент для создания adversarial данных в области обработки естественного языка (NLP). Позволяет:
- Генерировать adversarial примеры для атаки на NLP модели.
- Использовать эти примеры для тестирования и улучшения устойчивости моделей.

## Foolbox
Foolbox - библиотека для создания adversarial данных для атак на модели машинного обучения. Основные возможности:
- Создание и применение adversarial примеров для тестирования моделей.
- Оценка устойчивости моделей к различным видам атак.

## modelscan
modelscan - лаконичный инструмент для проверки моделей на наличие уязвимостей в области:
- Сериализации моделей.
- Инъекций кода и других уязвимостей безопасности.

## safetensors
safetensors - инструмент для безопасного сохранения весов моделей. Основные преимущества:
- Предотвращает случайные или преднамеренные модификации весов.
- Обеспечивает безопасность и целостность сохраненных данных.

## ml_privacy_meter
ml_privacy_meter - инструмент для оценки конфиденциальности моделей машинного обучения. **Пока не тестировал, слишком сложный инструмент для меня, нужно больше времени**

## mindgard cli
mindgard cli - инструмент с интеграцией в mlops пайплайн:
- Есть возможность установить максимальное кол-во запросов, которые мы хотим направлять в модель одновременно.
- Можно установить трешхолды на риски `--risk-threshold 50`.